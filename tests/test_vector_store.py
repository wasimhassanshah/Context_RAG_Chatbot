"""
Comprehensive Test Suite for LlamaIndex Vector Store
Location: tests/test_vector_store.py
"""

import sys
import os
import time
from pathlib import Path

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

from src.contextual_rag.rag.vector_store import VectorStoreManager, setup_vector_store_manager

class VectorStoreTester:
    """Comprehensive tester for LlamaIndex RAG system"""
    
    def __init__(self):
        print("ğŸ§ª Initializing Vector Store Tester...")
        self.test_results = {}
        self.manager = None
        
    def test_manager_initialization(self):
        """Test 1: Manager initialization and component setup"""
        print("\nğŸ“‹ Test 1: Manager Initialization")
        print("-" * 50)
        
        try:
            self.manager = setup_vector_store_manager()
            
            if self.manager:
                print("âœ… Vector store manager initialized")
                print(f"âœ… Processed data path: {self.manager.processed_data_path}")
                print(f"âœ… Embedding dimensions: {self.manager.embedding_dim}")
                
                # Check components
                components = {
                    'ollama_manager': self.manager.ollama_manager,
                    'embedding_model': self.manager.embedding_model,
                    'llm_model': self.manager.llm_model,
                    'vector_store': self.manager.vector_store
                }
                
                for name, component in components.items():
                    if component:
                        print(f"âœ… {name}: Initialized")
                    else:
                        print(f"âŒ {name}: Not initialized")
                
                self.test_results['initialization'] = {
                    'status': 'PASS',
                    'components_initialized': len([c for c in components.values() if c])
                }
            else:
                print("âŒ Failed to initialize manager")
                self.test_results['initialization'] = {
                    'status': 'FAIL',
                    'error': 'Manager is None'
                }
                
        except Exception as e:
            print(f"âŒ Initialization failed: {e}")
            self.test_results['initialization'] = {
                'status': 'FAIL',
                'error': str(e)
            }
    
    def test_database_connection(self):
        """Test 2: Database connection and configuration"""
        print("\nğŸ—„ï¸ Test 2: Database Connection")
        print("-" * 50)
        
        if not self.manager:
            print("âŒ Skipping - manager not initialized")
            self.test_results['database_connection'] = {'status': 'SKIP'}
            return
        
        try:
            # Test database stats
            stats = self.manager.get_database_stats()
            
            if 'error' in stats:
                print(f"âŒ Database connection failed: {stats['error']}")
                self.test_results['database_connection'] = {
                    'status': 'FAIL',
                    'error': stats['error']
                }
            else:
                print("âœ… Database connection successful")
                print(f"âœ… Table exists: {stats.get('table_exists', False)}")
                print(f"âœ… Current embeddings: {stats.get('total_embeddings', 0)}")
                print(f"âœ… Embedding dimension: {stats.get('embedding_dimension', 0)}")
                
                self.test_results['database_connection'] = {
                    'status': 'PASS',
                    'table_exists': stats.get('table_exists', False),
                    'current_embeddings': stats.get('total_embeddings', 0)
                }
                
        except Exception as e:
            print(f"âŒ Database test failed: {e}")
            self.test_results['database_connection'] = {
                'status': 'FAIL',
                'error': str(e)
            }
    
    def test_document_loading(self):
        """Test 3: Loading processed documents"""
        print("\nğŸ“š Test 3: Document Loading")
        print("-" * 50)
        
        if not self.manager:
            print("âŒ Skipping - manager not initialized")
            self.test_results['document_loading'] = {'status': 'SKIP'}
            return
        
        try:
            start_time = time.time()
            documents = self.manager.load_processed_documents()
            loading_time = time.time() - start_time
            
            print(f"âœ… Documents loaded: {len(documents)}")
            print(f"âœ… Loading time: {loading_time:.2f} seconds")
            
            if documents:
                # Show sample document info
                sample_doc = documents[0]
                print("âœ… Sample document:")
                print(f"   ğŸ“„ Text length: {len(sample_doc.text)} characters")
                print(f"   ğŸ“‹ Metadata keys: {list(sample_doc.metadata.keys())}")
                print(f"   ğŸ“„ Filename: {sample_doc.metadata.get('filename', 'Unknown')}")
                print(f"   ğŸ†” Document ID: {sample_doc.id_}")
                
                # Show distribution by document
                doc_counts = {}
                for doc in documents:
                    filename = doc.metadata.get('filename', 'Unknown')
                    doc_counts[filename] = doc_counts.get(filename, 0) + 1
                
                print("âœ… Document distribution:")
                for filename, count in doc_counts.items():
                    print(f"   ğŸ“„ {filename}: {count} chunks")
                
                self.test_results['document_loading'] = {
                    'status': 'PASS',
                    'total_documents': len(documents),
                    'loading_time': loading_time,
                    'document_distribution': doc_counts
                }
            else:
                print("âš ï¸ No documents loaded")
                self.test_results['document_loading'] = {
                    'status': 'PASS',
                    'total_documents': 0,
                    'reason': 'No processed documents found'
                }
                
        except Exception as e:
            print(f"âŒ Document loading failed: {e}")
            self.test_results['document_loading'] = {
                'status': 'FAIL',
                'error': str(e)
            }
    
    def test_embedding_generation(self):
        """Test 4: Embedding generation with Ollama"""
        print("\nğŸ”¢ Test 4: Embedding Generation")
        print("-" * 50)
        
        if not self.manager:
            print("âŒ Skipping - manager not initialized")
            self.test_results['embedding_generation'] = {'status': 'SKIP'}
            return
        
        try:
            # Test with different types of text
            test_texts = [
                "What are the procurement approval requirements?",
                "Information security policies and procedures",
                "HR employee onboarding process",
                "Budget allocation and financial controls"
            ]
            
            successful_embeddings = 0
            total_time = 0
            
            for i, text in enumerate(test_texts):
                print(f"   ğŸ§ª Test {i+1}: {text[:40]}...")
                
                start_time = time.time()
                success = self.manager.test_embedding_generation(text)
                embedding_time = time.time() - start_time
                
                if success:
                    successful_embeddings += 1
                    print(f"      âœ… Success ({embedding_time:.2f}s)")
                else:
                    print(f"      âŒ Failed ({embedding_time:.2f}s)")
                
                total_time += embedding_time
            
            avg_time = total_time / len(test_texts)
            success_rate = (successful_embeddings / len(test_texts)) * 100
            
            print("âœ… Embedding tests completed:")
            print(f"   ğŸ“Š Success rate: {success_rate:.1f}%")
            print(f"   â±ï¸ Average time: {avg_time:.2f}s per embedding")
            print(f"   ğŸ“ Embedding dimension: {self.manager.embedding_dim}")
            
            self.test_results['embedding_generation'] = {
                'status': 'PASS' if successful_embeddings > 0 else 'FAIL',
                'success_rate': success_rate,
                'average_time': avg_time,
                'successful_embeddings': successful_embeddings
            }
            
        except Exception as e:
            print(f"âŒ Embedding generation test failed: {e}")
            self.test_results['embedding_generation'] = {
                'status': 'FAIL',
                'error': str(e)
            }
    
    def test_index_creation(self):
        """Test 5: Vector index creation"""
        print("\nğŸ” Test 5: Vector Index Creation")
        print("-" * 50)
        
        if not self.manager:
            print("âŒ Skipping - manager not initialized")
            self.test_results['index_creation'] = {'status': 'SKIP'}
            return
        
        try:
            # Load documents first
            documents = self.manager.load_processed_documents()
            
            if not documents:
                print("âš ï¸ No documents to index")
                self.test_results['index_creation'] = {
                    'status': 'SKIP',
                    'reason': 'No documents available'
                }
                return
            
            start_time = time.time()
            index = self.manager.create_index(documents)
            indexing_time = time.time() - start_time
            
            if index:
                print("âœ… Index created successfully")
                print(f"âœ… Indexing time: {indexing_time:.2f} seconds")
                print(f"âœ… Documents per second: {len(documents)/indexing_time:.1f}")
                
                # Check database stats after indexing
                stats = self.manager.get_database_stats()
                print(f"âœ… Total embeddings in database: {stats.get('total_embeddings', 0)}")
                
                self.test_results['index_creation'] = {
                    'status': 'PASS',
                    'indexing_time': indexing_time,
                    'documents_indexed': len(documents),
                    'embeddings_created': stats.get('total_embeddings', 0)
                }
            else:
                print("âŒ Index creation failed")
                self.test_results['index_creation'] = {
                    'status': 'FAIL',
                    'error': 'Index is None'
                }
                
        except Exception as e:
            print(f"âŒ Index creation failed: {e}")
            self.test_results['index_creation'] = {
                'status': 'FAIL',
                'error': str(e)
            }
    
    def test_retrieval_system(self):
        """Test 6: Document retrieval system"""
        print("\nğŸ” Test 6: Document Retrieval")
        print("-" * 50)
        
        if not self.manager or not self.manager.index:
            print("âŒ Skipping - index not available")
            self.test_results['retrieval_system'] = {'status': 'SKIP'}
            return
        
        try:
            # Test queries related to your documents
            test_queries = [
                "What are the procurement approval requirements?",
                "Information security policies for employees",
                "HR policies for employee onboarding",
                "Budget approval process",
                "Document management procedures"
            ]
            
            retrieval_results = {}
            total_retrieval_time = 0
            
            for query in test_queries:
                print(f"\n   ğŸ” Query: {query}")
                
                start_time = time.time()
                results = self.manager.test_retrieval(query, top_k=3)
                retrieval_time = time.time() - start_time
                
                total_retrieval_time += retrieval_time
                
                if results:
                    print(f"      âœ… Retrieved {len(results)} documents ({retrieval_time:.2f}s)")
                    
                    # Show top result
                    top_result = results[0]
                    print(f"      ğŸ“„ Top result (score: {top_result['score']:.3f}):")
                    print(f"         {top_result['content'][:100]}...")
                    print(f"         Source: {top_result['metadata'].get('filename', 'Unknown')}")
                    
                    retrieval_results[query] = {
                        'results_count': len(results),
                        'top_score': top_result['score'],
                        'retrieval_time': retrieval_time
                    }
                else:
                    print(f"      âŒ No results found ({retrieval_time:.2f}s)")
                    retrieval_results[query] = {
                        'results_count': 0,
                        'retrieval_time': retrieval_time
                    }
            
            avg_retrieval_time = total_retrieval_time / len(test_queries)
            successful_queries = len([r for r in retrieval_results.values() if r['results_count'] > 0])
            
            print(f"\nâœ… Retrieval testing completed:")
            print(f"   ğŸ“Š Successful queries: {successful_queries}/{len(test_queries)}")
            print(f"   â±ï¸ Average retrieval time: {avg_retrieval_time:.2f}s")
            
            self.test_results['retrieval_system'] = {
                'status': 'PASS' if successful_queries > 0 else 'FAIL',
                'successful_queries': successful_queries,
                'total_queries': len(test_queries),
                'average_retrieval_time': avg_retrieval_time,
                'query_results': retrieval_results
            }
            
        except Exception as e:
            print(f"âŒ Retrieval testing failed: {e}")
            self.test_results['retrieval_system'] = {
                'status': 'FAIL',
                'error': str(e)
            }
    
    def test_llm_integration(self):
        """Test 7: LLM response generation"""
        print("\nğŸ¤– Test 7: LLM Integration")
        print("-" * 50)
        
        if not self.manager:
            print("âŒ Skipping - manager not initialized")
            self.test_results['llm_integration'] = {'status': 'SKIP'}
            return
        
        try:
            # Test query and context
            test_query = "What are the main procurement approval requirements?"
            
            # Get context from retrieval
            if self.manager.index:
                retrieved_docs = self.manager.test_retrieval(test_query, top_k=3)
                context_chunks = [doc['content'] for doc in retrieved_docs[:2]]  # Use top 2
            else:
                # Fallback context
                context_chunks = [
                    "Procurement approvals require department head approval for purchases up to $10,000.",
                    "All purchases above $10,000 require board approval and must follow standard procurement process."
                ]
            
            print(f"ğŸ” Query: {test_query}")
            print(f"ğŸ“„ Using {len(context_chunks)} context chunks")
            
            start_time = time.time()
            response = self.manager.test_llm_response(test_query, context_chunks)
            response_time = time.time() - start_time
            
            print(f"âœ… LLM response generated ({response_time:.2f}s)")
            print(f"ğŸ“ Response preview: {response[:200]}...")
            print(f"ğŸ“ Response length: {len(response)} characters")
            
            # Basic quality checks
            quality_checks = {
                'has_content': len(response.strip()) > 0,
                'mentions_approval': 'approval' in response.lower(),
                'reasonable_length': 50 < len(response) < 2000,
                'not_error_message': 'error' not in response.lower()[:50]
            }
            
            passed_checks = sum(quality_checks.values())
            total_checks = len(quality_checks)
            
            print(f"âœ… Quality checks: {passed_checks}/{total_checks} passed")
            for check, passed in quality_checks.items():
                status = "âœ…" if passed else "âŒ"
                print(f"   {status} {check}")
            
            self.test_results['llm_integration'] = {
                'status': 'PASS' if passed_checks >= total_checks * 0.75 else 'FAIL',
                'response_time': response_time,
                'response_length': len(response),
                'quality_score': passed_checks / total_checks,
                'quality_checks': quality_checks
            }
            
        except Exception as e:
            print(f"âŒ LLM integration test failed: {e}")
            self.test_results['llm_integration'] = {
                'status': 'FAIL',
                'error': str(e)
            }
    
    def run_all_tests(self):
        """Run all vector store tests"""
        print("ğŸ§ª LLAMAINDEX VECTOR STORE COMPREHENSIVE TESTING")
        print("=" * 60)
        
        # Run all tests in order
        self.test_manager_initialization()
        self.test_database_connection()
        self.test_document_loading()
        self.test_embedding_generation()
        self.test_index_creation()
        self.test_retrieval_system()
        self.test_llm_integration()
        
        # Print summary
        self.print_test_summary()
    
    def print_test_summary(self):
        """Print comprehensive test summary"""
        print("\nğŸ“Š TEST SUMMARY")
        print("=" * 60)
        
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results.values() if r['status'] == 'PASS'])
        skipped_tests = len([r for r in self.test_results.values() if r['status'] == 'SKIP'])
        failed_tests = total_tests - passed_tests - skipped_tests
        
        print(f"ğŸ¯ Total Tests: {total_tests}")
        print(f"âœ… Passed: {passed_tests}")
        print(f"â­ï¸ Skipped: {skipped_tests}")
        print(f"âŒ Failed: {failed_tests}")
        
        if total_tests > 0:
            success_rate = ((passed_tests + skipped_tests) / total_tests) * 100
            print(f"ğŸ“ˆ Success Rate: {success_rate:.1f}%")
        
        print("\nğŸ“‹ Detailed Results:")
        for test_name, result in self.test_results.items():
            status_emoji = "âœ…" if result['status'] == 'PASS' else "â­ï¸" if result['status'] == 'SKIP' else "âŒ"
            print(f"   {status_emoji} {test_name}: {result['status']}")
            
            # Show key metrics
            if 'total_documents' in result:
                print(f"      ğŸ“„ Documents: {result['total_documents']}")
            if 'embeddings_created' in result:
                print(f"      ğŸ”¢ Embeddings: {result['embeddings_created']}")
            if 'indexing_time' in result:
                print(f"      â±ï¸ Indexing: {result['indexing_time']:.2f}s")
            if 'average_retrieval_time' in result:
                print(f"      ğŸ” Retrieval: {result['average_retrieval_time']:.2f}s")
            if 'response_time' in result:
                print(f"      ğŸ¤– LLM: {result['response_time']:.2f}s")
        
        # Performance summary
        if 'index_creation' in self.test_results and self.test_results['index_creation']['status'] == 'PASS':
            indexing_result = self.test_results['index_creation']
            docs_indexed = indexing_result.get('documents_indexed', 0)
            indexing_time = indexing_result.get('indexing_time', 0)
            
            if docs_indexed > 0 and indexing_time > 0:
                print(f"\nâš¡ Performance Metrics:")
                print(f"   ğŸ“„ Documents indexed: {docs_indexed}")
                print(f"   â±ï¸ Total indexing time: {indexing_time:.2f}s")
                print(f"   ğŸ“ˆ Indexing rate: {docs_indexed/indexing_time:.1f} docs/second")
        
        # Final recommendation
        if failed_tests == 0:
            print("\nğŸ‰ ALL TESTS PASSED! RAG system is working perfectly!")
            if passed_tests >= 5:  # Core tests passed
                print("ğŸš€ Ready to proceed with Step 6: CrewAI Agent Orchestration!")
            else:
                print("âš ï¸ Some tests were skipped. System functional but may need optimization.")
        else:
            print(f"\nâš ï¸ {failed_tests} tests failed. Please fix the issues above.")
            print("ğŸ’¡ Common issues:")
            print("   - Check Ollama is running: ollama serve")
            print("   - Verify database connection")
            print("   - Ensure processed documents exist")

def main():
    """Main test function"""
    tester = VectorStoreTester()
    tester.run_all_tests()

if __name__ == "__main__":
    main()